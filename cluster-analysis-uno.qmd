---
title: "Exploratory Cluster Analysis for Josie Schafer"
author: "Michael Flynn, Prior Analytics, LLC."
fontsize: "11"
format: 
  html: default
  pdf: default
pdf-engine: pdflatex
---

```{r setup, include=FALSE}

library(tidyverse)
library(brms)
library(cluster)
library(factoextra)
library(dendextend)
library(ggdendro)
library(here)
library(tmaptools)
library(kableExtra)
```


# Cleaning Data

This first code block loads the data and performs any necessary cleaning, rescaling, etc.

First, there don't appear to be any missing values in any rows.

Second, for now I'm focusing primarily on broader demographic and institutional indicators for now, but also some more targeted variables that would likely help to explain disparate economic outcomes. For example, high-research universities granting PhDs or a higher number of community hospital beds. There are other variables that we could include that might be useful for some purposes (e.g. Medicare recipients by region) but I expect that these will be closely tracking other age-related demographic variables. 

Third, I'm rescaling the variables by dividing the observed value by the largest value of $X$ as follows: 

$$ X_i \over \max X $$

This puts all observed values on a $0-1$ scale. 

My understanding of clustering techniques is that when they calculate the distance between units, they will treat the scale of the variables equivalently. The idea here is to scale all of the cluster inputs so they are all on a $0-1$ scale, thereby treating all of them equivalently. That way variables with large values and large ranges don't dominate the clustering procedure.

That said, if there's reason to want to weight input variables differently for clustering we can explore that with more time. 


```{r data-cleaning}

# Read in data and select relevant variables for clusters. 
# Focus is on demographic and anchor institution variables.

# Read in raw data file
data <- readxl::read_xlsx(here("data/anchor regions analysis.xlsx"))

# List of variables to include in clustering
varlist <- c("totpop_19",  # Total pop
             "popchange",  # pop change
             "medage",     # Median age
             "labfor",     # percent population in labor force
             "pov",       # percent population living in poverty
             "poc",       # people of color as percent of pop
             "highed",     # Percent population with at least bachelor's 
             "forborn",    # Percent population foreign born
             "net_mig",    # Net domestic migration
             "highered_emp_qcew", 
             "highered_estab_qcew",
             "hospital_emp_qcew", 
             "hospital_estab_qcew",
             "inst_ipeds_enrollment_all", 
             "inst_ipeds_doctoralunihighrese", 
             "inst_ipeds_pellawards",
             "inst_hosp_ahacommunityhospitals",
             "inst_hosp_ahabeds",
             "inst_hosp_nihresearchfunding")

# Rescale the variables from 0-1
data.clean <- data |>
  mutate(across(all_of(varlist), # Variables to scale
                ~.x/max(.x),                                          # Scale relative to max
                .names = "{col}_max")) |>                             # Add "max" suffix
  dplyr::select(MSA, ends_with("_max")) |>                            # select chosen variables
  column_to_rownames("MSA")
```


# Clustering Methods

Here I start with agglomerative/hierarchical clustering methods. The goal as I understand it is to find a happy medium number of groups that illustrates the variability across regions and anchor institutions while still being tractable for analyses. 

The priority here is to construct clusters on the basis of 1) anchor institution characteristics, and 2) demographic characteristics of the surrounding region. For now I'll combine these into a single cluster, but we may want to think about constructing two clusters, one on the basis of demographic traits and the other on the basis of anchor institution traits. This would help parse out effects later if the client is interested in using these as predictors in subsequent regression analyses. 

I'm going to create a few different clusters and we can compare the characteristics and performance of each, and then choose which one the client likes best.

I chose the "complete" method for the `hclust()` function because it generates a better distribution of clusters than the other methods. For example, others tend to produce either very flat distributions, in which case you may just as well use dummy variables for each MSA or city, or they produce oddly concentrated clusters with 80-90% of observatiosn falling into cluster group 2.

```{r clustering-methods}

distance <- dist(data.clean) # calculate Euclidian distance between obs

hc.tree <- hclust(distance, method = "complete") # Create cluster groupings based on distance


# List of distances to use in generating clusters
cluster.size.list <- list("5" = 5, 
                          "10" = 10,
                          "15" = 15,
                          "20" = 20, 
                          "25" = 25, 
                          "30" = 30,
                          "35" = 35,
                          "40" = 40)

cluster.ids <- map(
  .x = seq_along(cluster.size.list),
  .f = ~ cutree(hc.tree, k = cluster.size.list[[.x]])
                   ) |> 
  bind_cols() 

names(cluster.ids) <- c("cluster_5", "cluster_10", "cluster_15", "cluster_20", "cluster_25", "cluster_30", "cluster_35", "cluster_40")

data.out <- data.clean |> 
  bind_cols(cluster.ids) 


```

# Choosing the optimal number of clusters

@fig-cluster-histograms shows the distribution of the observations depending on the number of clusters chosen. In general, 25--35 clusters seems like a nice balance between parsimony and too much detail. Smaller numbers of clusters, like 5 or 10, group too many areas together (see the spike at group #1). In general we see there are regularly spikes like these, but we start to get more variability as we move towards the 25--30 range.

```{r fig-cluster-histograms, message=FALSE}
#| label: fig-cluster-histograms
#| fig-cap: "Histograms showing the distribution of clusters depending on the number of clusters chosen."
cluster.list <- list("cluster_5", "cluster_10", "cluster_15", "cluster_20", "cluster_25", "cluster_30", "cluster_35", "cluster_40")



plot.out <- data.out |> 
  dplyr::select(starts_with("cluster")) |> 
  map2(
    .y = cluster.list,
    .f = ~{ggplot(data.out, aes(x = .)) +
    geom_histogram(bins = 40) +
        labs(title = .y)}
)


patchwork::wrap_plots(plot.out)

```


```{r cluster-table}


table.data <- data |> 
  bind_cols(cluster.ids) |> 
  dplyr::select(varlist, cluster_30) |> 
  group_by(cluster_30) |> 
  dplyr::summarise(across(everything(),
                          mean)) 

names(table.data) <- c("Total Population (2019)",
                      "Population Change",
                      "Median Age",
                      "% Population in Labor Force",
                      "% Population in Poverty",
                      "% Population People of Color",
                      "$% Population with Bachelor's Degree",
                      "% Population Foreign Born",
                      "Net Domestic Migration",
                      "Higher Education Employment",
                      "Higher Education Establishments",
                      "Hospital Employment",
                      "Hospital Establishments",
                      "Higher Education Enrollment",
                      "High Research Doctoral Degree Institutions",
                      "Total Pell Grant Amounts Awarded",
                      "Hospitals/Community Hospitals",
                      "Hospital Beds",
                      "NIH Research Funding",
                      "Cluster")



table.out <- table.data |> 
  kbl(longtable = TRUE) |> 
  kable_styling(font_size = 8) |> 
  scroll_box(height = "600px", width = "800px") 

table.out

```
