---
title: "Exploratory Cluster Analysis for Josie Schafer"
author: "Michael Flynn, Prior Analytics, LLC."
fontsize: "11"
format: 
  html: default
  pdf: default
pdf-engine: pdflatex
---

```{r setup, include=FALSE}

library(tidyverse)
library(brms)
library(cluster)
library(factoextra)
library(dendextend)
library(ggdendro)
library(here)
library(tmaptools)
library(kableExtra)
library(lme4)
library(modelsummary)
```


# Cleaning Data

This first code block loads the data and performs any necessary cleaning, rescaling, etc.

First, there don't appear to be any missing values in any rows.

Second, for now I'm focusing primarily on broader demographic and institutional indicators for now, but also some more targeted variables that would likely help to explain disparate economic outcomes. For example, high-research universities granting PhDs or a higher number of community hospital beds. There are other variables that we could include that might be useful for some purposes (e.g. Medicare recipients by region) but I expect that these will be closely tracking other age-related demographic variables. 

Third, I'm rescaling the variables by dividing the observed value by the largest value of $X$ as follows: 

$$ X_i \over \max X $$

This puts all observed values on a $0-1$ scale. 

My understanding of clustering techniques is that when they calculate the distance between units, they will treat the scale of the variables equivalently. The idea here is to scale all of the cluster inputs so they are all on a $0-1$ scale, thereby treating all of them equivalently. That way variables with large values and large ranges don't dominate the clustering procedure.

That said, if there's reason to want to weight input variables differently for clustering we can explore that with more time. 


```{r data-cleaning}

# Read in data and select relevant variables for clusters. 
# Focus is on demographic and anchor institution variables.

# Read in raw data file
data <- readxl::read_xlsx(here("data/anchor regions analysis.xlsx"))

# List of variables to include in clustering
varlist <- c("totpop_19",  # Total pop
             "popchange",  # pop change
             "medage",     # Median age
             "labfor",     # percent population in labor force
             "pov",       # percent population living in poverty
             "poc",       # people of color as percent of pop
             "highed",     # Percent population with at least bachelor's 
             "forborn",    # Percent population foreign born
             "net_mig",    # Net domestic migration
             "highered_emp_qcew", 
             "highered_estab_qcew",
             "hospital_emp_qcew", 
             "hospital_estab_qcew",
             "inst_ipeds_enrollment_all", 
             "inst_ipeds_doctoralunihighrese", 
             "inst_ipeds_pellawards",
             "inst_hosp_ahacommunityhospitals",
             "inst_hosp_ahabeds",
             "inst_hosp_nihresearchfunding")

# Rescale the variables from 0-1
data.clean <- data |>
  mutate(across(all_of(varlist), # Variables to scale
                ~.x/max(.x),                                          # Scale relative to max
                .names = "{col}_max")) |>                             # Add "max" suffix
  dplyr::select(MSA, ends_with("_max")) |>                            # select chosen variables
  column_to_rownames("MSA")
```


# Clustering Methods

Here I start with agglomerative/hierarchical clustering methods. The goal as I understand it is to find a happy medium number of groups that illustrates the variability across regions and anchor institutions while still being tractable for analyses. 

The priority here is to construct clusters on the basis of 1) anchor institution characteristics, and 2) demographic characteristics of the surrounding region. For now I'll combine these into a single cluster, but we may want to think about constructing two clusters, one on the basis of demographic traits and the other on the basis of anchor institution traits. This would help parse out effects later if the client is interested in using these as predictors in subsequent regression analyses. 

I'm going to create a few different clusters and we can compare the characteristics and performance of each, and then choose which one the client likes best.

I chose the "complete" method for the `hclust()` function because it generates a better distribution of clusters than the other methods. For example, others tend to produce either very flat distributions, in which case you may just as well use dummy variables for each MSA or city, or they produce oddly concentrated clusters with 80-90% of observations falling into cluster group 2.

```{r clustering-methods}

distance <- dist(data.clean) # calculate Euclidean distance between obs

hc.tree <- hclust(distance, method = "complete") # Create cluster groupings based on distance


# List of distances to use in generating clusters
cluster.size.list <- list("5" = 5, 
                          "10" = 10,
                          "15" = 15,
                          "20" = 20, 
                          "25" = 25, 
                          "30" = 30,
                          "35" = 35,
                          "40" = 40)

cluster.ids <- map(
  .x = seq_along(cluster.size.list),
  .f = ~ cutree(hc.tree, k = cluster.size.list[[.x]])
                   ) |> 
  bind_cols() 

names(cluster.ids) <- c("cluster_5", "cluster_10", "cluster_15", "cluster_20", "cluster_25", "cluster_30", "cluster_35", "cluster_40")

data.out <- data |> 
  bind_cols(cluster.ids) |> 
  dplyr::select(starts_with("cluster"), Name, State, MSA, varlist)

names(data.out) <- c("Cluster 5",
                     "Cluster 10",
                     "Cluster 15",
                     "Cluster 20",
                     "Cluster 25",
                     "Cluster 30",
                     "Cluster 35",
                     "Cluster 40",
                     "Name",
                     "State",
                     "MSA",
                     "Total Population (2019)",
                     "Population Change",
                     "Median Age",
                     "% Population in Labor Force",
                     "% Population in Poverty",
                     "% Population People of Color",
                     "$% Population with Bachelor's Degree",
                     "% Population Foreign Born",
                     "Net Domestic Migration",
                     "Higher Education Employment",
                     "Higher Education Establishments",
                     "Hospital Employment",
                     "Hospital Establishments",
                     "Higher Education Enrollment",
                     "High Research Doctoral Degree Institutions",
                     "Total Pell Grant Amounts Awarded",
                     "Hospitals/Community Hospitals",
                     "Hospital Beds",
                     "NIH Research Funding")

write_csv(data.out,
          here::here("data/raw-data-with-cluster-ids.csv"))

```

# Choosing the optimal number of clusters

@fig-cluster-histograms shows the distribution of the observations depending on the number of clusters chosen. In general, 25--35 clusters seems like a nice balance between parsimony and too much detail. Smaller numbers of clusters, like 5 or 10, group too many areas together (see the spike at group #1). In general we see there are regularly spikes like these, but we start to get more variability as we move towards the 25--30 range.

The "ideal" number of clusters will also depend on modeling considerations as I briefly address below. Depending on what the final models look like you may want to use a smaller number of clusters. At some point there's going to be a tradeoff between the total number of clusters and the value added with respect to model inputs. 

```{r fig-cluster-histograms, message=FALSE}
#| label: fig-cluster-histograms
#| fig-cap: "Histograms showing the distribution of clusters depending on the number of clusters chosen."
cluster.list <- list("cluster_5", "cluster_10", "cluster_15", "cluster_20", "cluster_25", "cluster_30", "cluster_35", "cluster_40")

plot.out <- data.out |> 
  dplyr::select(starts_with("cluster")) |> 
  map2(
    .y = cluster.list,
    .f = ~{ggplot(data.out, aes(x = .)) +
    geom_histogram(bins = 40) +
        labs(title = .y)}
)


patchwork::wrap_plots(plot.out)

```


```{r cluster-table}


table.data <- data |> 
  bind_cols(cluster.ids) |> 
  dplyr::select(varlist, cluster_30) |> 
  group_by(cluster_30) |> 
  dplyr::summarise(across(everything(),
                          mean)) 

# Save output for table to send to client.
write_csv(table.data,
          here::here("data/table-data-output"))

names(table.data) <- c("Cluster",
                       "Total Population (2019)",
                      "Population Change",
                      "Median Age",
                      "% Population in Labor Force",
                      "% Population in Poverty",
                      "% Population People of Color",
                      "$% Population with Bachelor's Degree",
                      "% Population Foreign Born",
                      "Net Domestic Migration",
                      "Higher Education Employment",
                      "Higher Education Establishments",
                      "Hospital Employment",
                      "Hospital Establishments",
                      "Higher Education Enrollment",
                      "High Research Doctoral Degree Institutions",
                      "Total Pell Grant Amounts Awarded",
                      "Hospitals/Community Hospitals",
                      "Hospital Beds",
                      "NIH Research Funding")



table.out <- table.data |> 
  kbl(longtable = TRUE) |> 
  kable_styling(font_size = 8) |> 
  scroll_box(height = "600px", width = "800px") 

table.out

```



# Modeling Exploration

Here I'm just running a few models that look at how the clusters perform in predicting outcomes of interest. Again, this is something we can revisit given more time and some discussion to inject more domain knowledge into things. 

One issue to consider is whether or not the final data will ultimately have more than ~350 observations. 25--35 dummy indicator variables and possibly various other covariates may be a lot relative to the total number of observations.

I use the GDP index and per capita income here because they should facilitate a pretty simple linear model. The residual checks at the end provide some basic support for this. They're not perfectly normally distributed, so in the future some adjustments to the models would be helpful to provide better fit.

```{r linear-model-block}

model.data <- data |> 
  bind_cols(cluster.ids)

# GDP index models
m1 <- lm(index_real_gdp_21 ~ factor(cluster_10), data = model.data)
m2 <- lm(index_real_gdp_21 ~ factor(cluster_20), data = model.data)
m3 <- lm(index_real_gdp_21 ~ factor(cluster_30), data = model.data)
m4 <- lm(index_real_gdp_21 ~ factor(cluster_40), data = model.data)

# Per Capita Income models
m5 <- lm(percapita_personal_income_21 ~ factor(cluster_10), data = model.data)
m6 <- lm(percapita_personal_income_21 ~ factor(cluster_20), data = model.data)
m7 <- lm(percapita_personal_income_21 ~ factor(cluster_30), data = model.data)
m8 <- lm(percapita_personal_income_21 ~ factor(cluster_40), data = model.data)


mlist <- list(m1, m2, m3, m4, m5, m6, m7, m8)

modelsummary(mlist,
             fmt = 3,
             stars = TRUE,
             estimate = "estimate",
             statistic = "std.error",
             title = "Linear Regression Models",
             output = "kableExtra") |> 
  kable_styling("striped") |> 
  add_header_above(c(" " = 1, "2021 GDP Index (1-4)" = 4, "2021 Per Capita Income (5-8)" = 4))
  


```


```{r multilevel-model-block}
# GDP index models
mm1 <- lmer(index_real_gdp_21 ~ factor(cluster_10) + (1|State), data = model.data)
mm2 <- lmer(index_real_gdp_21 ~ factor(cluster_20) + (1|State), data = model.data)
mm3 <- lmer(index_real_gdp_21 ~ factor(cluster_30) + (1|State), data = model.data)
mm4 <- lmer(index_real_gdp_21 ~ factor(cluster_40) + (1|State), data = model.data)

# Per Capita Income models
mm5 <- lmer(percapita_personal_income_21 ~ factor(cluster_10) + (1|State), data = model.data)
mm6 <- lmer(percapita_personal_income_21 ~ factor(cluster_20) + (1|State), data = model.data)
mm7 <- lmer(percapita_personal_income_21 ~ factor(cluster_30) + (1|State), data = model.data)
mm8 <- lmer(percapita_personal_income_21 ~ factor(cluster_40) + (1|State), data = model.data)


mmlist <- list(mm1, mm2, mm3, mm4, mm5, mm6, mm7, mm8)

modelsummary(mmlist,
             fmt = 3,
             stars = TRUE,
             estimate = "estimate",
             statistic = "std.error",
             title = "Multilevel Regression Models",
             notes = c("State used as grouping term."),
             output = "kableExtra") |> 
  kable_styling("striped") |> 
  add_header_above(c(" " = 1, "2021 GDP Index (1-4)" = 4, "2021 Per Capita Income (5-8)" = 4))
  


```


# Residual Check

## Linear Regression
```{r residual-check-lm}

residual.list.lm <- map(
  .x = seq_along(mlist),
  .f = ~ mlist[[.x]]$residuals
)


plot.out <- map(
  .x = residual.list.lm,
  .f = ~ggplot(data = as.data.frame(.x), aes(x = .)) +
      geom_histogram(bins = 40)
)


patchwork::wrap_plots(plot.out)
```

## Multilevel Model

The multilevel models look similar but there's generally less dispersion in the residuals as compared to the simple linear model.

```{r residual-check-mm}

residual.list.mm <- map(
  .x = seq_along(mmlist),
  .f = ~ residuals(mmlist[[.x]])
)


plot.out <- map(
  .x = residual.list.mm,
  .f = ~ggplot(data = as.data.frame(.x), aes(x = .)) +
      geom_histogram(bins = 40)
)


patchwork::wrap_plots(plot.out)
```
